{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ih9HiJVHoEc"
      },
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QeLo9Bq_Rqf"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BigBirdForMaskedLM\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "# from transformers import AutoModelForSequenceClassification\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e_TBDMplwiC"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuAdCBOZf_RJ"
      },
      "source": [
        "### Specify Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYmc_3RzHbR2"
      },
      "source": [
        "Dataset = \"WikiText-2\" \n",
        "# Dataset = \"Enwik8\"\n",
        "# Dataset = \"PennTreeBank\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feQdfYOYga4_"
      },
      "source": [
        "### Specify Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiTpwam0gfD9"
      },
      "source": [
        "MAX_SEQ_LENGTH = 160\n",
        "BATCH_SIZE = 128\n",
        "BLOCK_SIZE = 16\n",
        "NUM_RANDOM_BLOCKS = 2\n",
        "LR = 2e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "EPOCHS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_61jHoylwiE"
      },
      "source": [
        "# Fine-tuning pretrained BigBird model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-stYP6rlwiF"
      },
      "source": [
        "## Preparing the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDWvsgcclwiI"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\", use_fast=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained('google/reformer-crime-and-punishment', use_fast=True)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYlbSFSpHEfi"
      },
      "source": [
        "def tokenize_function_ptb(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "def tokenize_function_wikitext2_enwik8(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "def prep_enwik8(path):\n",
        "    file = path + 'enwik8'\n",
        "    \n",
        "    # Read file\n",
        "    with open(file) as f:\n",
        "        lines = f.readlines()\n",
        "        \n",
        "    random.shuffle(lines)\n",
        "\n",
        "    # Calculate splits: 80/10/10 - train/val/test\n",
        "    train_split = math.floor(len(lines)*.8)\n",
        "    test_val_split = math.floor(len(lines)*.1)\n",
        "    \n",
        "    with open(path+'enwik8_train.txt', 'w') as train:\n",
        "        with open(path+'enwik8_validation.txt', 'w') as val:\n",
        "            with open(path+'enwik8_test.txt', 'w') as test:\n",
        "                for i, line in enumerate(lines):\n",
        "                    if i < train_split:\n",
        "                        train.write(line)\n",
        "                    elif i < train_split + test_val_split:\n",
        "                        val.write(line)\n",
        "                    else:\n",
        "                        test.write(line)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VVDJ9oXgOc1"
      },
      "source": [
        "### Get tokenized datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoooIUhOlwiH"
      },
      "source": [
        "if Dataset == \"WikiText-2\":\n",
        "  raw_datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "  tokenized_datasets = raw_datasets.map(tokenize_function_wikitext2_enwik8, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "elif Dataset == \"PennTreeBank\":\n",
        "  raw_datasets = load_dataset(\"ptb_text_only\")\n",
        "  tokenized_datasets = raw_datasets.map(tokenize_function_ptb, batched=True, num_proc=4, remove_columns=[\"sentence\"])\n",
        "elif Dataset == \"Enwik8\":\n",
        "  !wget https://data.deepai.org/enwik8.zip\n",
        "  !unzip -qq 'tiny-imagenet-200.zip'\n",
        "  prep_enwik8('/content/')\n",
        "  datasets = load_dataset('text', data_files={'train': '/content/enwik8_train.txt','validation': '/content/enwik8_validation.txt','test': '/content/enwik8_test.txt'})\n",
        "  tokenized_datasets = datasets.map(tokenize_function_wikitext2_enwik8, batched=True, num_proc=4, remove_columns=[\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDY6eTKngRBj"
      },
      "source": [
        "### Group text with a max sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q42mFdhkxJ2P"
      },
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    if total_length >= MAX_SEQ_LENGTH:\n",
        "        total_length = (total_length // MAX_SEQ_LENGTH) * MAX_SEQ_LENGTH\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + max_seq_length] for i in range(0, total_length, MAX_SEQ_LENGTH)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9HB_oyCqi7Z"
      },
      "source": [
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_proc=4,\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKhgibC4lwiM"
      },
      "source": [
        "### Next, load the pretrained model from the checkpoint and fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63q6tPx0hDb0"
      },
      "source": [
        "## clear cache\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm5Q8DTmlwiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f621c1-aea6-4346-89f9-06d68d5f6f28"
      },
      "source": [
        "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "model_checkpoint = \"google/bigbird-roberta-base\"\n",
        "model = BigBirdForMaskedLM.from_pretrained(model_checkpoint, num_random_blocks=NUM_RANDOM_BLOCKS, block_size=BLOCK_SIZE).to(device)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHKJdqQAwAX1"
      },
      "source": [
        "## Specify training arguments\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "training_args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-wikitext2\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=LR,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    # no_cuda = True,\n",
        "    # load_best_model_at_end=True,\n",
        "    # push_to_hub=True,\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOUW-F6ZzIqo"
      },
      "source": [
        "## Create the Trainer object to train using the API\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeU3_eNszN5P"
      },
      "source": [
        "train_results = trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKYvAsTNJb67"
      },
      "source": [
        "trainer.save_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToiH9OMHhzWA"
      },
      "source": [
        "### Print results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D5bMjQ8C8e7"
      },
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLjMa3ZaDNyL"
      },
      "source": [
        "# Display Metrics\n",
        "metrics = train_results.metrics\n",
        "trainer.log_metrics(\"train\", metrics)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}