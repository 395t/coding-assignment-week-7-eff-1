{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1iBYa0J1bnWiWYg4CaKsAp4CDSB1NkppJ",
      "authorship_tag": "ABX9TyOKFgl+3wiNQLt5/wSb2Kb8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Co02gPMO3F"
      },
      "source": [
        "# Causal Language Modeling Task\n",
        "A series of experiments demonstrating causal language modeling and training performance on a Reformer model. Model, datasets, and examples sourced from Huggingface.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Test Models\n",
        "**Reformer**\n",
        "* 6-layer\n",
        "* 256-hidden\n",
        "* 2-heads\n",
        "* 3M parameters\n",
        "* Trained on English text: Crime and Punishment novel by Fyodor Dostoyevsky.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj2u4Bg9fDTV"
      },
      "source": [
        "!pip install datasets transformers sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRta5DTgfU8R"
      },
      "source": [
        "# Imports\n",
        "import math, random, torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlAgz9W7fZ0d"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwxau1G_WgRX"
      },
      "source": [
        "def tokenize_function_ptb(examples):\n",
        "    return tokenizer(examples[\"sentence\"])\n",
        "\n",
        "def tokenize_function_wt2_enwik8(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "    \n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "def prep_enwik8(path):\n",
        "    file = path + 'enwik8'\n",
        "    \n",
        "    # Read file\n",
        "    with open(file) as f:\n",
        "        lines = f.readlines()\n",
        "        \n",
        "    random.shuffle(lines)\n",
        "\n",
        "    # Calculate splits: 80/10/10 - train/val/test\n",
        "    train_split = math.floor(len(lines)*.8)\n",
        "    test_val_split = math.floor(len(lines)*.1)\n",
        "    \n",
        "    with open(path+'enwik8_train.txt', 'w') as train:\n",
        "        with open(path+'enwik8_validation.txt', 'w') as val:\n",
        "            with open(path+'enwik8_test.txt', 'w') as test:\n",
        "                for i, line in enumerate(lines):\n",
        "                    if i < train_split:\n",
        "                        train.write(line)\n",
        "                    elif i < train_split + test_val_split:\n",
        "                        val.write(line)\n",
        "                    else:\n",
        "                        test.write(line)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZdL_GvaE2G3"
      },
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE = 2e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 16\n",
        "block_size = 16384\n",
        "PUSH_HUB = False\n",
        "AXIAL_POS = False\n",
        "\n",
        "# Dataset selection\n",
        "DATASET_SELECT = 0  # 0 = wikitext-2, 1 = penn treebank, 2 = enwik8\n",
        "PATH_TO_ENWIK8 = '/content/data/' "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry42glJfbhKA"
      },
      "source": [
        "model_id = 'google/reformer-crime-and-punishment'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzVnHy_bWGi_"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, padding=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2J9CNzsV7p6"
      },
      "source": [
        "if DATASET_SELECT == 0:\n",
        "  datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "  tokenized_datasets = datasets.map(tokenize_function_wt2_enwik8, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "elif DATASET_SELECT == 1:\n",
        "  datasets = load_dataset(\"ptb_text_only\")\n",
        "  tokenized_datasets = datasets.map(tokenize_function_ptb, batched=True, num_proc=4, remove_columns=[\"sentence\"])\n",
        "elif DATASET_SELECT == 2:\n",
        "  prep_enwik8(PATH_TO_ENWIK8)\n",
        "  datasets = load_dataset('text', data_files={'train': PATH_TO_ENWIK8+'enwik8_train.txt','validation': PATH_TO_ENWIK8+'enwik8_validation.txt','test': PATH_TO_ENWIK8+'enwik8_test.txt'})\n",
        "  tokenized_datasets = datasets.map(tokenize_function_wt2_enwik8, batched=True, num_proc=4, remove_columns=[\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyv662ShWw7P"
      },
      "source": [
        "#tokenized_datasets[\"train\"][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKLwzMHDY1HL"
      },
      "source": [
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PIKvfK1ZGCO"
      },
      "source": [
        "#tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVPinWb5asBf"
      },
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id, axial_pos_embds=AXIAL_POS).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppAKZYbIbCST"
      },
      "source": [
        "model_name = model_id.split(\"/\")[-1]\n",
        "training_args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-wikitext2\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    adafactor=True,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    push_to_hub=PUSH_HUB,\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biV7vRcdbj-A"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"validation\"],\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUa9seqNbmy-"
      },
      "source": [
        "train_results = trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7x4CCxmgq5h"
      },
      "source": [
        "#trainer.save_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok8Mi_2nUod0"
      },
      "source": [
        "# Print Perplexity\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQG6z7Oxg1Bt"
      },
      "source": [
        "# Display Metrics\n",
        "metrics = train_results.metrics\n",
        "trainer.log_metrics(\"train\", metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwAI257-bev6"
      },
      "source": [
        "# Memory Benchmarks\n",
        "## Test Models\n",
        "**Reformer**\n",
        "* 6-layer\n",
        "* 256-hidden\n",
        "* 2-heads\n",
        "* 3M parameters\n",
        "* Trained on English text: Crime and Punishment novel by Fyodor Dostoyevsky.  \n",
        "\n",
        "**Reformer**\n",
        "* 12-layer\n",
        "* 1024-hidden\n",
        "* 8-heads\n",
        "* 149M parameters\n",
        "* Trained on English Wikipedia data - enwik8.\n",
        "\n",
        "**GPT2 Base**\n",
        "* 12-layer\n",
        "* 768-hidden\n",
        "* 12-heads\n",
        "* 117M parameters\n",
        "* OpenAI GPT-2 English model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUOvJSQ_bdyO"
      },
      "source": [
        "!pip -qq install transformers py3nvml"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjaCQWrOfkfR"
      },
      "source": [
        "from transformers import GPT2Config, ReformerConfig, PyTorchBenchmark, PyTorchBenchmarkArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqaGj771cBk1"
      },
      "source": [
        "config_cp_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_cp_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(64, 192), axial_pos_shape=(512, 1024))  # enable axial positional embeddings\n",
        "\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[256, 512, 2048, 16384], batch_sizes=[16], models=[\"Reformer C/P\"], speed=False, env_print=False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8alurX1fdTl4"
      },
      "source": [
        "benchmark = PyTorchBenchmark(configs=[config_cp_no_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSQcARYufYNE"
      },
      "source": [
        "benchmark = PyTorchBenchmark(configs=[config_cp_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MKW1ymiyjCr"
      },
      "source": [
        "config_en8_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[256, 512, 1024, 2048], batch_sizes=[16], models=[\"Reformer en8\"], speed=False, env_print=False)\n",
        "\n",
        "benchmark = PyTorchBenchmark(configs=[config_en8_no_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_pNPJ5gviw"
      },
      "source": [
        "config_gpt2 = GPT2Config.from_pretrained(\"gpt2\")\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[256, 512, 1024, 2048], batch_sizes=[16], models=[\"GPT2 Base\"], speed=False, env_print=False)\n",
        "\n",
        "benchmark = PyTorchBenchmark(configs=[config_gpt2], args=benchmark_args)\n",
        "result = benchmark.run()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
